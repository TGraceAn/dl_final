# -*- coding: utf-8 -*-
"""Another copy of vision_transformer_experiment2_train_from_scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16DRcxPKG1alrc0Er4LPLqllhmSLRo3jR
"""

import torch
#dataset link: https://www.robots.ox.ac.uk/~vgg/data/pets/ || https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz || https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz
from torchvision.datasets import OxfordIIITPet
import matplotlib.pyplot as plt
from random import random
from torchvision.transforms import Resize, ToTensor
from torchvision.transforms.functional import to_pil_image

to_tensor = [Resize((224, 224)), ToTensor()] #pre-processing

to_tensor

import torchvision
from torchvision import transforms
import PIL
transforms = [
    Resize((224, 224)),
    # transforms.RandomCrop(50, padding=1),
    # transforms.RandomGrayscale(p=0.1),
    # transforms.RandomHorizontalFlip(),
    # transforms.RandomRotation(20),
    # transforms.RandomRotation(degrees=(0, 20)),
    transforms.AutoAugment(),
    ToTensor()
]

print(transforms)

class Compose(object):
    def __init__(self, transforms):
        self.transforms = transforms

    def __call__(self, image, target):
        for t in self.transforms:
            image = t(image)
        return image, target

def show_images(images, num_samples=40, cols=8):
    """ Plots some samples from the dataset """
    plt.figure(figsize=(15,15))
    idx = int(len(dataset) / num_samples)
    print(images)
    for i, img in enumerate(images):
        if i % idx == 0:
            plt.subplot(int(num_samples/cols) + 1, cols, int(i/idx) + 1)
            plt.imshow(to_pil_image(img[0]))

dataset = OxfordIIITPet(root=".", download=True, transforms=Compose(to_tensor))
# new_dataset = OxfordIIITPet(root=".", download=True, transforms=Compose(transforms))

# show_images(dataset)

from torchvision.models import vit_b_16

from torch.utils.data import DataLoader
from torch.utils.data import random_split

train_split = int(0.8 * len(dataset))
train, test = random_split(dataset, [train_split, len(dataset) - train_split])

train_dataloader = DataLoader(train, batch_size=32, shuffle=True)
test_dataloader = DataLoader(test, batch_size=32, shuffle=True)

import torch.optim as optim
from torch import nn
from torch import Tensor
import numpy as np


device = "cuda"

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.model = vit_b_16(weights='DEFAULT')
        self.model.heads = nn.Linear(768, 37)
        self.model.heads.weight.data.normal_(mean=0.5, std=0.5)
        self.model.heads.bias.data.zero_()

    def forward(self, x: torch.Tensor):
            # Reshape and permute the input tensor
            x = self.model._process_input(x)
            n = x.shape[0]

            # Expand the class token to the full batch
            batch_class_token = self.model.class_token.expand(n, -1, -1)
            x = torch.cat([batch_class_token, x], dim=1)

            x = self.model.encoder(x)

            # Classifier "token" as used by standard language architectures
            x = x[:, 0]

            x = self.model.heads(x)

            return x

model = Net().to(device)
# print(model)

optimizer = optim.AdamW(model.parameters(), lr=0.01, betas=(0.9, 0.999), weight_decay = 0.1)
criterion = nn.CrossEntropyLoss()

test_loss = 10
best_loss = 10

# #use for the test loss calculation
# epoch_losses_test = []

for epoch in range(100):
    epoch_losses = []
    model.train()
    for step, (inputs, labels) in enumerate(train_dataloader):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        #get training loss values
        epoch_losses.append(loss.item())


    if epoch % 5 == 0:
        #calculating training loss
        print(f">>> Epoch {epoch} train loss: ", np.mean(epoch_losses))

        # #get the last_test_lost
        # last_test_loss = np.mean(epoch_losses_test)

        #initialize for this one
        epoch_losses_test = []

        model.eval()

        for step, (inputs, labels) in enumerate(test_dataloader):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            #get this test lost values
            epoch_losses_test.append(loss.item())

        #calculating testloss
        test_loss = np.mean(epoch_losses_test)

        print(f">>> Epoch {epoch} test loss: ", test_loss)

        #saving files
        if(best_loss > test_loss):
              torch.save(model.state_dict(), 'best.pth')
              torch.save(model, 'model_best.pth')
              print("Model saved best ckpt")
              best_loss = test_loss

        torch.save(model.state_dict(), 'last.pth')
        torch.save(model, 'model_last.pth')
        print("Model saved last ckpt")

model.eval()
inputs, labels = next(iter(train_dataloader))
inputs, labels = inputs.to(device), labels.to(device)
outputs = model(inputs)


print("Predicted classes", outputs.argmax(-1))
print("Actual classes", labels)

inputs, labels = next(iter(test_dataloader))
inputs, labels = inputs.to(device), labels.to(device)
outputs = model(inputs)


print("Predicted classes", outputs.argmax(-1))
print("Actual classes", labels)

model.load_state_dict(torch.load('best.pth'))
inputs, labels = next(iter(train_dataloader))
inputs, labels = inputs.to(device), labels.to(device)
outputs = model(inputs)


print("Predicted classes", outputs.argmax(-1))
print("Actual classes", labels)

inputs, labels = next(iter(test_dataloader))
inputs, labels = inputs.to(device), labels.to(device)
outputs = model(inputs)


print("Predicted classes", outputs.argmax(-1))
print("Actual classes", labels)